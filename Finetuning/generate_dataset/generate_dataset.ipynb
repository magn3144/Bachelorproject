{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_websites import download_websites\n",
    "from generate_scraping_tasks import generate_scraping_tasks\n",
    "from generate_solution_code import generate_solution_code\n",
    "from extract_html_info import extract_relevant_information\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "google play.html does not exist\n",
      "--------------------\n",
      "zeromike.html does not exist\n",
      "--------------------\n",
      "Generating data for woman\n",
      "Extracted HTML elements for woman\n",
      "Generated scraping tasks for woman\n",
      "Generated solutions for woman\n",
      "--------------------\n",
      "Generating data for wordpress\n",
      "Extracted HTML elements for wordpress\n",
      "Generated scraping tasks for wordpress\n",
      "Generated solutions for wordpress\n",
      "--------------------\n",
      "Generating data for bloggersroad\n",
      "Extracted HTML elements for bloggersroad\n",
      "Generated scraping tasks for bloggersroad\n",
      "Generated solutions for bloggersroad\n",
      "--------------------\n",
      "Generating data for bog & ide\n",
      "Extracted HTML elements for bog & ide\n",
      "Generated scraping tasks for bog & ide\n",
      "Generated solutions for bog & ide\n",
      "--------------------\n",
      "Generating data for globestudios\n",
      "Extracted HTML elements for globestudios\n",
      "Generated scraping tasks for globestudios\n",
      "Generated solutions for globestudios\n",
      "--------------------\n",
      "Generating data for h&m\n",
      "Extracted HTML elements for h&m\n",
      "Generated scraping tasks for h&m\n",
      "Generated solutions for h&m\n",
      "--------------------\n",
      "jem&fix.html does not exist\n",
      "--------------------\n",
      "Generating data for top\n",
      "Extracted HTML elements for top\n",
      "Generated scraping tasks for top\n"
     ]
    },
    {
     "ename": "Timeout",
     "evalue": "Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    462\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1376\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\urllib3\\connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\urllib3\\util\\retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 550\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[0;32m    551\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\urllib3\\packages\\six.py:770\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 770\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[0;32m    771\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\urllib3\\connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 468\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_timeout(err\u001b[39m=\u001b[39;49me, url\u001b[39m=\u001b[39;49murl, timeout_value\u001b[39m=\u001b[39;49mread_timeout)\n\u001b[0;32m    469\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\urllib3\\connectionpool.py:357\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[1;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(err, SocketTimeout):\n\u001b[1;32m--> 357\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeoutError(\n\u001b[0;32m    358\u001b[0m         \u001b[39mself\u001b[39m, url, \u001b[39m\"\u001b[39m\u001b[39mRead timed out. (read timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m timeout_value\n\u001b[0;32m    359\u001b[0m     )\n\u001b[0;32m    361\u001b[0m \u001b[39m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[39m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\openai\\api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    597\u001b[0m         method,\n\u001b[0;32m    598\u001b[0m         abs_url,\n\u001b[0;32m    599\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    600\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    601\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    602\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    603\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[0;32m    604\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\requests\\adapters.py:532\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[1;32m--> 532\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeout(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    533\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[1;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\magnu\\Documents\\GitHub Projects\\Bachelorproject\\Finetuning\\generate_dataset\\generate_dataset.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/generate_dataset.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j, task_name \u001b[39min\u001b[39;00m scraping_tasks\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/generate_dataset.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         task_name \u001b[39m=\u001b[39m task_name[\u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/generate_dataset.ipynb#W1sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         generate_solution_code(website, HTML_file, category, HTML_elements, task_name, j)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/generate_dataset.ipynb#W1sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGenerated solutions for \u001b[39m\u001b[39m{\u001b[39;00mwebsite\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/generate_dataset.ipynb#W1sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m--------------------\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\magnu\\Documents\\GitHub Projects\\Bachelorproject\\Finetuning\\generate_dataset\\generate_solution_code.py:26\u001b[0m, in \u001b[0;36mgenerate_solution_code\u001b[1;34m(website, HTML_file, category, HTML_string, task, task_i)\u001b[0m\n\u001b[0;32m     23\u001b[0m user_prompt \u001b[39m=\u001b[39m user_prompt\u001b[39m.\u001b[39mformat(website\u001b[39m=\u001b[39mwebsite, HTML_file\u001b[39m=\u001b[39mHTML_file, category\u001b[39m=\u001b[39mcategory, HTML_string\u001b[39m=\u001b[39mHTML_string, task\u001b[39m=\u001b[39mtask)\n\u001b[0;32m     25\u001b[0m \u001b[39m# Get response from GPT-4\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m response \u001b[39m=\u001b[39m get_response(system_prompt, user_prompt, \u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Extract the code from the response\u001b[39;00m\n\u001b[0;32m     29\u001b[0m scraper_code \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(?<=```python\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn)[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS]+?(?=\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn```)\u001b[39m\u001b[39m\"\u001b[39m, response)\n",
      "File \u001b[1;32mc:\\Users\\magnu\\Documents\\GitHub Projects\\Bachelorproject\\Finetuning\\generate_dataset\\generate_response.py:6\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(system_prompt, user_prompt, model)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_response\u001b[39m(system_prompt, user_prompt, model):\n\u001b[1;32m----> 6\u001b[0m     completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      7\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      8\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[0;32m      9\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: system_prompt},\n\u001b[0;32m     10\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: user_prompt}\n\u001b[0;32m     11\u001b[0m         ]\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\openai\\api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[0;32m    291\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    292\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    293\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    294\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    295\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[0;32m    298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\magnu\\anaconda3\\envs\\BP\\lib\\site-packages\\openai\\api_requestor.py:607\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    596\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrequest(\n\u001b[0;32m    597\u001b[0m         method,\n\u001b[0;32m    598\u001b[0m         abs_url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         proxies\u001b[39m=\u001b[39m_thread_context\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mproxies,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 607\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    609\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mAPIConnectionError(\n\u001b[0;32m    610\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError communicating with OpenAI: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)\n\u001b[0;32m    611\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mTimeout\u001b[0m: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)"
     ]
    }
   ],
   "source": [
    "websites_csv = 'websites_mi.csv'\n",
    "save_folder = 'downloaded_pages'\n",
    "ROWS = (0, 10000)\n",
    "NUM_TASKS_PER_WEBSITE = 10\n",
    "\n",
    "# Download websites\n",
    "# download_websites(websites_csv, save_folder)\n",
    "\n",
    "# Generate scraping tasks and solutions\n",
    "websites = pd.read_csv(websites_csv)\n",
    "\n",
    "for i, row in websites.iterrows():\n",
    "    if i < ROWS[0] or i > ROWS[1]:\n",
    "        continue\n",
    "    \n",
    "    print(\"--------------------\")\n",
    "\n",
    "    category, website, link = row\n",
    "    HTML_file = f'{save_folder}/{website}.html'\n",
    "\n",
    "    if os.path.exists(f'solution_code/{website}'):\n",
    "        print(f'{website} already has data')\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(HTML_file):\n",
    "        print(f'{website}.html does not exist')\n",
    "        continue\n",
    "\n",
    "    print(f\"Generating data for {website}\")\n",
    "\n",
    "    HTML_elements = extract_relevant_information(HTML_file)\n",
    "    with open(f'extracted_info/{website}.txt', 'w') as f:\n",
    "        f.write(HTML_elements)\n",
    "    \n",
    "    print(f'Extracted HTML elements for {website}')\n",
    "\n",
    "    generate_scraping_tasks(link, website, category, HTML_elements, NUM_TASKS_PER_WEBSITE)\n",
    "    with open(f'scraping_tasks/{website}.txt', 'r') as f:\n",
    "        scraping_tasks = pd.DataFrame([{'task': task} for task in f.readlines()])\n",
    "    \n",
    "    print(f'Generated scraping tasks for {website}')\n",
    "\n",
    "    for j, task_name in scraping_tasks.iterrows():\n",
    "        task_name = task_name['task']\n",
    "        generate_solution_code(website, HTML_file, category, HTML_elements, task_name, j)\n",
    "     \n",
    "    print(f'Generated solutions for {website}')\n",
    "\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc_weather does not have generated data\n",
      "edx does not have generated data\n",
      "etsy does not have generated data\n",
      "avsforum does not have generated data\n",
      "aliexpress does not have generated data\n",
      "accuweather does not have generated data\n",
      "ebay does not have generated data\n",
      "coursera does not have generated data\n",
      "cnn does not have generated data\n",
      "nytimes does not have generated data\n",
      "nfl does not have generated data\n",
      "wunderground does not have generated data\n",
      "udemy does not have generated data\n",
      "target does not have generated data\n",
      "foreca does not have generated data\n",
      "theguardian does not have generated data\n",
      "mitocw.mit.edu does not have generated data\n",
      "snagajob does not have generated data\n",
      "yale.edu does not have generated data\n",
      "finance.yahoo does not have generated data\n",
      "khanacademy does not have generated data\n",
      "superpages does not have generated data\n",
      "weather does not have generated data\n",
      "snapchat does not have generated data\n",
      "goal does not have generated data\n",
      "reuters does not have generated data\n",
      "jstor does not have generated data\n",
      "twitter does not have generated data\n",
      "flickr does not have generated data\n",
      "usa does not have generated data\n",
      "nba does not have generated data\n",
      "whitepages does not have generated data\n",
      "yellowbook does not have generated data\n",
      "meteoblue does not have generated data\n",
      "simplyhired does not have generated data\n",
      "trulia does not have generated data\n",
      "pinterest does not have generated data\n",
      "slickdeals does not have generated data\n",
      "tripadvisor does not have generated data\n",
      "reuters does not have generated data\n",
      "zillow does not have generated data\n",
      "forbes does not have generated data\n",
      "skysports does not have generated data\n",
      "stackexchange does not have generated data\n",
      "walmart does not have generated data\n",
      "nature does not have generated data\n",
      "usajobs does not have generated data\n",
      "hotfrog does not have generated data\n",
      "realtor does not have generated data\n",
      "youtube does not have generated data\n",
      "zacks does not have generated data\n",
      "investing does not have generated data\n",
      "mlb does not have generated data\n",
      "metoffice does not have generated data\n",
      "yelp does not have generated data\n",
      "fool does not have generated data\n",
      "newegg does not have generated data\n",
      "sec does not have generated data\n",
      "homes does not have generated data\n",
      "quora does not have generated data\n",
      "weather does not have generated data\n",
      "weatherbug does not have generated data\n",
      "stanfordonline.stanford does not have generated data\n",
      "huffpost does not have generated data\n",
      "monster does not have generated data\n",
      "spoke does not have generated data\n",
      "marketwatch does not have generated data\n",
      "yell does not have generated data\n",
      "glassdoor does not have generated data\n",
      "remax does not have generated data\n",
      "loopnet does not have generated data\n",
      "instagram does not have generated data\n",
      "nhl does not have generated data\n",
      "nasa does not have generated data\n",
      "linkedin does not have generated data\n",
      "Dataset length: 384\n"
     ]
    }
   ],
   "source": [
    "# Combine all scraping tasks and solutions into one file\n",
    "\n",
    "# Load prompts\n",
    "system_prompt = open('prompts/generate_solution_code_system_prompt.txt', 'r').read()\n",
    "user_prompt = open('prompts/generate_solution_code_user_prompt.txt', 'r').read()\n",
    "\n",
    "# Load extracted info\n",
    "extracted_info = {}\n",
    "for filename in os.listdir('extracted_info'):\n",
    "    with open(f'extracted_info/{filename}', 'r') as f:\n",
    "        extracted_info[filename.split('.')[0]] = f.read()\n",
    "    \n",
    "# Load scraping tasks\n",
    "scraping_tasks = {}\n",
    "for filename in os.listdir('scraping_tasks'):\n",
    "    with open(f'scraping_tasks/{filename}', 'r') as f:\n",
    "        scraping_tasks[filename.split('.')[0]] = f.readlines()\n",
    "\n",
    "# Load solutions\n",
    "solutions = {}\n",
    "for folder in os.listdir('solution_code'):\n",
    "    solutions[folder] = {}\n",
    "    for filename in os.listdir(f'solution_code/{folder}'):\n",
    "        with open(f'solution_code/{folder}/{filename}', 'r') as f:\n",
    "            solutions[folder][filename.split('.')[0]] = f.read()\n",
    "\n",
    "# Load website infos\n",
    "websites = pd.read_csv('websites.csv')\n",
    "\n",
    "# Generate prompts\n",
    "prompts = {} # Dictionary of dictionaries\n",
    "categories = {}\n",
    "links = {}\n",
    "for i, row in websites.iterrows():\n",
    "    category, website, link = row\n",
    "    categories[website] = category\n",
    "    links[website] = link\n",
    "    if website not in scraping_tasks:\n",
    "        print(f'{website} does not have generated data')\n",
    "        continue\n",
    "    scraping_tasks_website = scraping_tasks[website]\n",
    "    HTML_file = f'downloaded_pages/{website}.html'\n",
    "    prompts[website] = {}\n",
    "    for j, scraping_task in enumerate(scraping_tasks_website):\n",
    "        scraping_task = scraping_task.strip()\n",
    "        solution = solutions[website][f'{website}_{j}']\n",
    "        HTML_elements = extracted_info[website]\n",
    "        prompts[website][f'{website}_{j}'] = user_prompt.format(website=website, HTML_file=HTML_file, category=category, HTML_string=HTML_elements, task=scraping_task)\n",
    "\n",
    "# Generate training samples\n",
    "training_samples = {}\n",
    "sample_template = \"\"\"### System:\n",
    "{system_prompt}\n",
    "\n",
    "### User:\n",
    "{user_prompt}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "{response}\n",
    "```\n",
    "\"\"\"\n",
    "system_prompt = open('prompts/generate_solution_code_system_prompt.txt', 'r').read()\n",
    "for website in prompts:\n",
    "    training_samples[website] = {}\n",
    "    for task_name in prompts[website]:\n",
    "        sample = sample_template.format(system_prompt=system_prompt, user_prompt=prompts[website][task_name], response=solutions[website][task_name])\n",
    "        training_samples[website][task_name] = sample\n",
    "\n",
    "# Convert prompts, solutions and training samples to lists\n",
    "websites_list = []\n",
    "task_names_list = []\n",
    "categories_list = []\n",
    "links_list = []\n",
    "prompts_list = []\n",
    "solutions_list = []\n",
    "training_samples_list = []\n",
    "for website in prompts:\n",
    "    for task_name in prompts[website]:\n",
    "        websites_list.append(website)\n",
    "        task_names_list.append(task_name)\n",
    "        categories_list.append(categories[website])\n",
    "        links_list.append(links[website])\n",
    "        prompts_list.append(prompts[website][task_name])\n",
    "        solutions_list.append(solutions[website][task_name])\n",
    "        training_samples_list.append(training_samples[website][task_name])\n",
    "\n",
    "# Combine to a list of dictionaries\n",
    "data = []\n",
    "for i in range(len(prompts_list)):\n",
    "    data.append({\n",
    "        'website': websites_list[i],\n",
    "        'task': task_names_list[i],\n",
    "        'category': categories_list[i],\n",
    "        'link': links_list[i],\n",
    "        'prompt': prompts_list[i],\n",
    "        'solution': solutions_list[i],\n",
    "        'training_sample': training_samples_list[i]\n",
    "    })\n",
    "print(\"Dataset length:\", len(data))\n",
    "\n",
    "# Save prompts, solutions and training samples as a json file\n",
    "# where each 'row' is a prompt, solution and training sample\n",
    "# for a particular website and task\n",
    "with open('dataset.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample: ### System:\n",
      "When asked to write a script, then write just the code, and nothing else. Don't write any explanation, comments, or disclaimers.\n",
      "\n",
      "### User:\n",
      "You are given a web page, the category of the page, randomly selected html elements on that page, the local path to the HTML file that should be scraped and a web-scraping task that you should solve.\n",
      "\n",
      "Here are some randomly selected HTML elements (containing text), and their corresponding XPaths from the target page:\n",
      "<title>(83) Update on $50k NVDA Puts : wallstreetbets</title>\n",
      "/html/head/title\n",
      "----------------\n",
      "<span>Flipping at the Grand Exchange</span>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[2]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\n",
      "----------------\n",
      "<span class=\"_1RIl585IYPW6cmNXwgRz0J\">User account menu</span>\n",
      "/html/body/div[1]/div/div[2]/div[1]/header/div/div[2]/div[2]/div/div[2]/button/span[2]\n",
      "----------------\n",
      "<a class=\"_3t5uN8xUmg0TOwRCOGQEcU\">r/wallstreetbets</a> sir, not \n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[47]/div/div/div/div[2]/div[2]/div[2]/div/p[1]/a[1]\n",
      "----------------\n",
      "<a class=\"zrXDKcys3Vl7vt1f6ef4V\">Privacy Policy</a> .\n",
      "/html/body/div[1]/div/div[2]/div[3]/div/section/div/section[1]/div/span[3]/a[2]\n",
      "----------------\n",
      "<div class=\"tbIApBd2DM_drfZQJjIum\">Screenshots of Social Media Posts are Prohibited</div>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[2]/div/div[2]/div\n",
      "----------------\n",
      "<div class=\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\">1</div>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[213]/div/div/div/div[2]/div[2]/div[3]/div[1]/div\n",
      "----------------\n",
      "<h1 class=\"_eYtD2XCVieq6emjKBH3m _2SdHzo12ISmrC8H86TgSCp _29WrubtjAcKqzJSPdQqQ4h\">Update on $50k NVDA Puts</h1>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[1]/div/div/div/div[3]/div[1]/div/h1\n",
      "----------------\n",
      "<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Lol that chip ban was a goddamn lifeboat from the </p>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[167]/div/div/div/div[2]/div[2]/div[2]/div/p\n",
      "----------------\n",
      "<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Cramer says Calls</p>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[173]/div/div/div/div[2]/div[2]/div[2]/div/p[1]\n",
      "----------------\n",
      "<td>MSFT</td>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[7]/td[8]\n",
      "----------------\n",
      "<h2 class=\"_eYtD2XCVieq6emjKBH3m\">About Community</h2>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[1]/div[1]/div[1]/h2\n",
      "----------------\n",
      "<th>-</th>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/thead/tr/th[5]\n",
      "----------------\n",
      "<span class=\"_1RIl585IYPW6cmNXwgRz0J\">Search within r/wallstreetbets</span>\n",
      "/html/body/div[1]/div/div[2]/div[1]/header/div/div[1]/div[3]/div/form/label/span\n",
      "----------------\n",
      "<span class=\"reddit-actionButton\">Tip</span>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[205]/div/div/div/div[2]/div[2]/div[3]/div[2]/div[2]/template/button/span[2]/span\n",
      "----------------\n",
      "<a class=\"_1WUTKdOO96akYfbq4CK6z6\">View discussions in 1 other community</a>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[4]/a\n",
      "----------------\n",
      "<a class=\"wM6scouPXXsFDSZmZPHRo DjcdNGtVXPcxG0yiFXIoZ _23wugcdiaj44hdfugIAlnX\">Unknownirish</a>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[3]/div/div/div/div[2]/div[2]/div[1]/span/div/div/div/a\n",
      "----------------\n",
      "<div class=\"tbIApBd2DM_drfZQJjIum\">Only Crypto Allowed is BTC and ETH</div>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[8]/div/div[2]/div\n",
      "----------------\n",
      "<div class=\"ULWj94BYSOqoJDetxgcnU\">u/ShopBitter</div>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[2]/div[2]/a/div\n",
      "----------------\n",
      "<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">How do you even give gold now? I don’t see it anyw</p>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[15]/div/div/div/div[2]/div[2]/div[2]/div/p\n",
      "----------------\n",
      "<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Nice</p>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[209]/div/div/div/div[2]/div[2]/div[2]/div/p\n",
      "----------------\n",
      "<td>453</td>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[2]/td[2]\n",
      "----------------\n",
      "<h2 class=\"_eYtD2XCVieq6emjKBH3m\">Moderators</h2>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[1]/div/h2\n",
      "----------------\n",
      "<span>Ask me to rap (WSB's Discount Tupac)</span>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[8]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\n",
      "----------------\n",
      "<span class=\"reddit-actionButton\">Tip</span>\n",
      "Do NOT include these directly in your code!\n",
      "\n",
      "The page is from this website: reddit\n",
      "\n",
      "The local path to the HTML file is downloaded_pages/reddit.html\n",
      "\n",
      "The category is: Social Media\n",
      "\n",
      "The task is: Extract all the titles from the discussion posts and save them in a CSV file.\n",
      "\n",
      "Now generate a python script that solves this task.\n",
      "Make sure the script works when I run it, without any modifications.\n",
      "The script you generate should always save the scraped data as a CSV file with the name 'scraped_data.csv'.\n",
      "You can use the given HTML elements above to gain information about the page.\n",
      "\n",
      "### Response:\n",
      "```\n",
      "import csv\n",
      "from lxml import etree\n",
      "\n",
      "def extract_titles(html_file):\n",
      "    tree = etree.parse(html_file)\n",
      "    titles = tree.xpath(\"//h3[@class='title']/a/text()\")\n",
      "    return titles\n",
      "\n",
      "def save_to_csv(data, csv_file):\n",
      "    with open(csv_file, 'w', newline='') as file:\n",
      "        writer = csv.writer(file)\n",
      "        writer.writerow(['Title'])\n",
      "        writer.writerows(data)\n",
      "\n",
      "html_file = 'downloaded_pages/reddit.html'\n",
      "csv_file = 'scraped_data.csv'\n",
      "\n",
      "titles = extract_titles(html_file)\n",
      "save_to_csv(titles, csv_file)\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_data = json.load(open('dataset.json', 'r'))\n",
    "# print(\"Loaded data length:\", len(loaded_data))\n",
    "# print(\"Website:\", loaded_data[0]['website'])\n",
    "# print(\"Task:\", loaded_data[0]['task'])\n",
    "# print(\"Category:\", loaded_data[0]['category'])\n",
    "# print(\"Link:\", loaded_data[0]['link'])\n",
    "# print(\"Prompt:\\n\" + loaded_data[0]['prompt'])\n",
    "# print(\"Solution:\", loaded_data[0]['solution'])\n",
    "print(\"Training sample:\", loaded_data[0]['training_sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'You are given a web page, the category of the page, randomly selected html elements on that page, the local path to the HTML file that should be scraped and a web-scraping task that you should solve.\\n\\nHere are some randomly selected HTML elements (containing text), and their corresponding XPaths from the target page:\\n<title>(83) Update on $50k NVDA Puts : wallstreetbets</title>\\n/html/head/title\\n----------------\\n<span>Flipping at the Grand Exchange</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[2]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\\n----------------\\n<span class=\"_1RIl585IYPW6cmNXwgRz0J\">User account menu</span>\\n/html/body/div[1]/div/div[2]/div[1]/header/div/div[2]/div[2]/div/div[2]/button/span[2]\\n----------------\\n<a class=\"_3t5uN8xUmg0TOwRCOGQEcU\">r/wallstreetbets</a> sir, not \\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[47]/div/div/div/div[2]/div[2]/div[2]/div/p[1]/a[1]\\n----------------\\n<a class=\"zrXDKcys3Vl7vt1f6ef4V\">Privacy Policy</a> .\\n/html/body/div[1]/div/div[2]/div[3]/div/section/div/section[1]/div/span[3]/a[2]\\n----------------\\n<div class=\"tbIApBd2DM_drfZQJjIum\">Screenshots of Social Media Posts are Prohibited</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[2]/div/div[2]/div\\n----------------\\n<div class=\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\">1</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[213]/div/div/div/div[2]/div[2]/div[3]/div[1]/div\\n----------------\\n<h1 class=\"_eYtD2XCVieq6emjKBH3m _2SdHzo12ISmrC8H86TgSCp _29WrubtjAcKqzJSPdQqQ4h\">Update on $50k NVDA Puts</h1>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[1]/div/div/div/div[3]/div[1]/div/h1\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Lol that chip ban was a goddamn lifeboat from the </p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[167]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Cramer says Calls</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[173]/div/div/div/div[2]/div[2]/div[2]/div/p[1]\\n----------------\\n<td>MSFT</td>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[7]/td[8]\\n----------------\\n<h2 class=\"_eYtD2XCVieq6emjKBH3m\">About Community</h2>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[1]/div[1]/div[1]/h2\\n----------------\\n<th>-</th>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/thead/tr/th[5]\\n----------------\\n<span class=\"_1RIl585IYPW6cmNXwgRz0J\">Search within r/wallstreetbets</span>\\n/html/body/div[1]/div/div[2]/div[1]/header/div/div[1]/div[3]/div/form/label/span\\n----------------\\n<span class=\"reddit-actionButton\">Tip</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[205]/div/div/div/div[2]/div[2]/div[3]/div[2]/div[2]/template/button/span[2]/span\\n----------------\\n<a class=\"_1WUTKdOO96akYfbq4CK6z6\">View discussions in 1 other community</a>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[4]/a\\n----------------\\n<a class=\"wM6scouPXXsFDSZmZPHRo DjcdNGtVXPcxG0yiFXIoZ _23wugcdiaj44hdfugIAlnX\">Unknownirish</a>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[3]/div/div/div/div[2]/div[2]/div[1]/span/div/div/div/a\\n----------------\\n<div class=\"tbIApBd2DM_drfZQJjIum\">Only Crypto Allowed is BTC and ETH</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[8]/div/div[2]/div\\n----------------\\n<div class=\"ULWj94BYSOqoJDetxgcnU\">u/ShopBitter</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[2]/div[2]/a/div\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">How do you even give gold now? I don’t see it anyw</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[15]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Nice</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[209]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<td>453</td>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[2]/td[2]\\n----------------\\n<h2 class=\"_eYtD2XCVieq6emjKBH3m\">Moderators</h2>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[1]/div/h2\\n----------------\\n<span>Ask me to rap (WSB\\'s Discount Tupac)</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[8]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\\n----------------\\n<span class=\"reddit-actionButton\">Tip</span>\\nDo NOT include these directly in your code!\\n\\nThe page is from this website: reddit\\n\\nThe local path to the HTML file is downloaded_pages/reddit.html\\n\\nThe category is: Forums and Review Sites\\n\\nThe task is: Extract all the titles from the discussion posts and save them in a CSV file.\\n\\nNow generate a python script that solves this task.\\nMake sure the script works when I run it, without any modifications.\\nThe script you generate should always save the scraped data as a CSV file with the name \\'scraped_data.csv\\'.\\nYou can use the given HTML elements above to gain information about the page.',\n",
       " 'solution': 'import csv\\nfrom lxml import etree\\n\\ndef extract_titles(html_file):\\n    tree = etree.parse(html_file)\\n    titles = tree.xpath(\"//h3[@class=\\'title\\']/a/text()\")\\n    return titles\\n\\ndef save_to_csv(data, csv_file):\\n    with open(csv_file, \\'w\\', newline=\\'\\') as file:\\n        writer = csv.writer(file)\\n        writer.writerow([\\'Title\\'])\\n        writer.writerows(data)\\n\\nhtml_file = \\'downloaded_pages/reddit.html\\'\\ncsv_file = \\'scraped_data.csv\\'\\n\\ntitles = extract_titles(html_file)\\nsave_to_csv(titles, csv_file)',\n",
       " 'training_sample': '### System:\\nWhen asked to write a script, then write just the code, and nothing else. Don\\'t write any explanation, comments, or disclaimers.\\n\\n### User:\\nYou are given a web page, the category of the page, randomly selected html elements on that page, the local path to the HTML file that should be scraped and a web-scraping task that you should solve.\\n\\nHere are some randomly selected HTML elements (containing text), and their corresponding XPaths from the target page:\\n<title>(83) Update on $50k NVDA Puts : wallstreetbets</title>\\n/html/head/title\\n----------------\\n<span>Flipping at the Grand Exchange</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[2]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\\n----------------\\n<span class=\"_1RIl585IYPW6cmNXwgRz0J\">User account menu</span>\\n/html/body/div[1]/div/div[2]/div[1]/header/div/div[2]/div[2]/div/div[2]/button/span[2]\\n----------------\\n<a class=\"_3t5uN8xUmg0TOwRCOGQEcU\">r/wallstreetbets</a> sir, not \\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[47]/div/div/div/div[2]/div[2]/div[2]/div/p[1]/a[1]\\n----------------\\n<a class=\"zrXDKcys3Vl7vt1f6ef4V\">Privacy Policy</a> .\\n/html/body/div[1]/div/div[2]/div[3]/div/section/div/section[1]/div/span[3]/a[2]\\n----------------\\n<div class=\"tbIApBd2DM_drfZQJjIum\">Screenshots of Social Media Posts are Prohibited</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[2]/div/div[2]/div\\n----------------\\n<div class=\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\">1</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[213]/div/div/div/div[2]/div[2]/div[3]/div[1]/div\\n----------------\\n<h1 class=\"_eYtD2XCVieq6emjKBH3m _2SdHzo12ISmrC8H86TgSCp _29WrubtjAcKqzJSPdQqQ4h\">Update on $50k NVDA Puts</h1>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[1]/div/div/div/div[3]/div[1]/div/h1\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Lol that chip ban was a goddamn lifeboat from the </p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[167]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Cramer says Calls</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[173]/div/div/div/div[2]/div[2]/div[2]/div/p[1]\\n----------------\\n<td>MSFT</td>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[7]/td[8]\\n----------------\\n<h2 class=\"_eYtD2XCVieq6emjKBH3m\">About Community</h2>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[1]/div[1]/div[1]/h2\\n----------------\\n<th>-</th>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/thead/tr/th[5]\\n----------------\\n<span class=\"_1RIl585IYPW6cmNXwgRz0J\">Search within r/wallstreetbets</span>\\n/html/body/div[1]/div/div[2]/div[1]/header/div/div[1]/div[3]/div/form/label/span\\n----------------\\n<span class=\"reddit-actionButton\">Tip</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[205]/div/div/div/div[2]/div[2]/div[3]/div[2]/div[2]/template/button/span[2]/span\\n----------------\\n<a class=\"_1WUTKdOO96akYfbq4CK6z6\">View discussions in 1 other community</a>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[4]/a\\n----------------\\n<a class=\"wM6scouPXXsFDSZmZPHRo DjcdNGtVXPcxG0yiFXIoZ _23wugcdiaj44hdfugIAlnX\">Unknownirish</a>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[3]/div/div/div/div[2]/div[2]/div[1]/span/div/div/div/a\\n----------------\\n<div class=\"tbIApBd2DM_drfZQJjIum\">Only Crypto Allowed is BTC and ETH</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[8]/div/div[2]/div\\n----------------\\n<div class=\"ULWj94BYSOqoJDetxgcnU\">u/ShopBitter</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[2]/div[2]/a/div\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">How do you even give gold now? I don’t see it anyw</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[15]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Nice</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[209]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<td>453</td>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[2]/td[2]\\n----------------\\n<h2 class=\"_eYtD2XCVieq6emjKBH3m\">Moderators</h2>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[1]/div/h2\\n----------------\\n<span>Ask me to rap (WSB\\'s Discount Tupac)</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[8]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\\n----------------\\n<span class=\"reddit-actionButton\">Tip</span>\\nDo NOT include these directly in your code!\\n\\nThe page is from this website: reddit\\n\\nThe local path to the HTML file is downloaded_pages/reddit.html\\n\\nThe category is: Forums and Review Sites\\n\\nThe task is: Extract all the titles from the discussion posts and save them in a CSV file.\\n\\nNow generate a python script that solves this task.\\nMake sure the script works when I run it, without any modifications.\\nThe script you generate should always save the scraped data as a CSV file with the name \\'scraped_data.csv\\'.\\nYou can use the given HTML elements above to gain information about the page.\\n\\n### Response:\\n```\\nimport csv\\nfrom lxml import etree\\n\\ndef extract_titles(html_file):\\n    tree = etree.parse(html_file)\\n    titles = tree.xpath(\"//h3[@class=\\'title\\']/a/text()\")\\n    return titles\\n\\ndef save_to_csv(data, csv_file):\\n    with open(csv_file, \\'w\\', newline=\\'\\') as file:\\n        writer = csv.writer(file)\\n        writer.writerow([\\'Title\\'])\\n        writer.writerows(data)\\n\\nhtml_file = \\'downloaded_pages/reddit.html\\'\\ncsv_file = \\'scraped_data.csv\\'\\n\\ntitles = extract_titles(html_file)\\nsave_to_csv(titles, csv_file)\\n```\\n'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
