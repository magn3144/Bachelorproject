{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_websites import download_websites\n",
    "from generate_scraping_tasks import generate_scraping_tasks, get_body\n",
    "from generate_solution_code import generate_solution_code\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_csv = 'websites.csv'\n",
    "save_folder = 'downloaded_pages'\n",
    "NUM_TASKS_PER_WEBSITE = 2\n",
    "\n",
    "# Download websites\n",
    "# download_websites(websites_csv, save_folder)\n",
    "\n",
    "# Generate scraping tasks\n",
    "websites = pd.read_csv(websites_csv)\n",
    "for _, row in websites.iterrows():\n",
    "    category, website, page, link = row\n",
    "    HTML_file = f'{save_folder}/{website}.html'\n",
    "    HTML_body = get_body(HTML_file)[:1000]\n",
    "    generate_scraping_tasks(link, website, category, HTML_body, NUM_TASKS_PER_WEBSITE)\n",
    "    with open(f'scraping_tasks/{website}.txt', 'r') as f:\n",
    "        scraping_tasks = pd.DataFrame([{'task': task} for task in f.readlines()])\n",
    "\n",
    "    for i, task in scraping_tasks.iterrows():\n",
    "        task = task['task']\n",
    "        generate_solution_code(website, HTML_file, category, HTML_body, task, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import amazon_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span aria-hidden=\"true\">$39.99</span>\n",
      "/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[1]/div/div/select/option[23]\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "from io import StringIO\n",
    "\n",
    "def extract_text_objects(html_content):\n",
    "    # Parse the HTML using lxml\n",
    "    parser = etree.HTMLParser()\n",
    "    tree = etree.parse(StringIO(html_content), parser)\n",
    "    \n",
    "    # Get all elements that have text directly inside them\n",
    "    elements_with_text = [elem for elem in tree.xpath('//*') if elem.text and not elem.getchildren()]\n",
    "\n",
    "    # Exclude script and style tags\n",
    "    elements_with_text = [elem for elem in elements_with_text if elem.tag not in ['script', 'style']]\n",
    "    \n",
    "    # Extract the HTML and xpath for each element\n",
    "    htmls = [etree.tostring(elem, encoding='unicode') for elem in elements_with_text]\n",
    "    xpaths = [tree.getpath(elem) for elem in elements_with_text]\n",
    "    \n",
    "    return htmls, xpaths\n",
    "\n",
    "html = open('downloaded_pages/amazon.html', 'r').read()\n",
    "htmls, xpaths = extract_text_objects(html)\n",
    "print(htmls[300])\n",
    "print(xpaths[30])\n",
    "print(len(htmls))\n",
    "\n",
    "# TODO: Make a function that generates a snippet of some of the HTML and XPaths.\n",
    "# They should be selected equally for example or randomly.\n",
    "# Also if some of them are too long then they should be truncated.\n",
    "# Also remove all other HTML options from the objects except for class and id.\n",
    "# If the text is too long it should be truncated.\n",
    "# Include both objects with originally long text and short text.\n",
    "# Include objects with as many different tags as possible, fx div, p, span, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
