{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_websites import download_websites\n",
    "from generate_scraping_tasks import generate_scraping_tasks\n",
    "from generate_solution_code import generate_solution_code\n",
    "from extract_html_info import extract_relevant_information\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "reddit already has data\n",
      "--------------------\n",
      "homefinder already has data\n",
      "--------------------\n",
      "espn already has data\n",
      "--------------------\n",
      "washingtonpost already has data\n",
      "--------------------\n",
      "merchantcircle already has data\n",
      "--------------------\n",
      "census already has data\n",
      "--------------------\n",
      "ppubs already has data\n",
      "--------------------\n",
      "tumblr already has data\n",
      "--------------------\n",
      "nasdaq already has data\n",
      "--------------------\n",
      "finviz already has data\n",
      "--------------------\n",
      "aljazeera already has data\n",
      "--------------------\n",
      "indeed already has data\n",
      "--------------------\n",
      "foxnews already has data\n",
      "--------------------\n",
      "seekingalpha already has data\n",
      "--------------------\n",
      "careerbuilder already has data\n",
      "--------------------\n",
      "redfin already has data\n",
      "--------------------\n",
      "ziprecruiter already has data\n",
      "--------------------\n",
      "bestbuy already has data\n",
      "--------------------\n",
      "fifa already has data\n",
      "--------------------\n",
      "aboutus already has data\n",
      "--------------------\n",
      "data already has data\n",
      "--------------------\n",
      "boardgamegeek already has data\n",
      "--------------------\n",
      "bodybuilding already has data\n",
      "--------------------\n",
      "bleacherreport already has data\n",
      "--------------------\n",
      "cbsports already has data\n",
      "--------------------\n",
      "century21 already has data\n",
      "--------------------\n",
      "amazon already has data\n",
      "--------------------\n",
      "bloomberg already has data\n",
      "--------------------\n",
      "almanac already has data\n",
      "--------------------\n",
      "britannica already has data\n",
      "--------------------\n",
      "alibaba already has data\n",
      "--------------------\n",
      "dice already has data\n",
      "--------------------\n",
      "bbc already has data\n",
      "--------------------\n",
      "fbi already has data\n",
      "--------------------\n",
      "city-data already has data\n",
      "--------------------\n",
      "bbc_weather already has data\n",
      "--------------------\n",
      "edx already has data\n",
      "--------------------\n",
      "etsy already has data\n",
      "--------------------\n",
      "avsforum already has data\n",
      "--------------------\n",
      "aliexpress already has data\n",
      "--------------------\n",
      "accuweather already has data\n",
      "--------------------\n",
      "ebay already has data\n",
      "--------------------\n",
      "Generating data for coursera\n",
      "Extracted HTML elements for coursera\n",
      "Generated scraping tasks for coursera\n",
      "Generated solutions for coursera\n",
      "--------------------\n",
      "Generating data for cnn\n",
      "Extracted HTML elements for cnn\n",
      "Generated scraping tasks for cnn\n",
      "Generated solutions for cnn\n",
      "--------------------\n",
      "Generating data for nytimes\n",
      "Extracted HTML elements for nytimes\n",
      "Generated scraping tasks for nytimes\n",
      "Generated solutions for nytimes\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "websites_csv = 'websites.csv'\n",
    "save_folder = 'downloaded_pages'\n",
    "ROWS = (0, 44)\n",
    "NUM_TASKS_PER_WEBSITE = 10\n",
    "\n",
    "# Download websites\n",
    "# download_websites(websites_csv, save_folder)\n",
    "\n",
    "# Generate scraping tasks and solutions\n",
    "websites = pd.read_csv(websites_csv)\n",
    "\n",
    "for i, row in websites.iterrows():\n",
    "    if i < ROWS[0] or i > ROWS[1]:\n",
    "        continue\n",
    "    \n",
    "    print(\"--------------------\")\n",
    "\n",
    "    category, website, link = row\n",
    "    HTML_file = f'{save_folder}/{website}.html'\n",
    "\n",
    "    if os.path.exists(f'solution_code/{website}'):\n",
    "        print(f'{website} already has data')\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(HTML_file):\n",
    "        print(f'{website}.html does not exist')\n",
    "        continue\n",
    "\n",
    "    print(f\"Generating data for {website}\")\n",
    "\n",
    "    HTML_elements = extract_relevant_information(HTML_file)\n",
    "    with open(f'extracted_info/{website}.txt', 'w') as f:\n",
    "        f.write(HTML_elements)\n",
    "    \n",
    "    print(f'Extracted HTML elements for {website}')\n",
    "\n",
    "    generate_scraping_tasks(link, website, category, HTML_elements, NUM_TASKS_PER_WEBSITE)\n",
    "    with open(f'scraping_tasks/{website}.txt', 'r') as f:\n",
    "        scraping_tasks = pd.DataFrame([{'task': task} for task in f.readlines()])\n",
    "    \n",
    "    print(f'Generated scraping tasks for {website}')\n",
    "\n",
    "    for j, task_name in scraping_tasks.iterrows():\n",
    "        task_name = task_name['task']\n",
    "        generate_solution_code(website, HTML_file, category, HTML_elements, task_name, j)\n",
    "     \n",
    "    print(f'Generated solutions for {website}')\n",
    "\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc_weather does not have generated data\n",
      "edx does not have generated data\n",
      "etsy does not have generated data\n",
      "avsforum does not have generated data\n",
      "aliexpress does not have generated data\n",
      "accuweather does not have generated data\n",
      "ebay does not have generated data\n",
      "coursera does not have generated data\n",
      "cnn does not have generated data\n",
      "nytimes does not have generated data\n",
      "nfl does not have generated data\n",
      "wunderground does not have generated data\n",
      "udemy does not have generated data\n",
      "target does not have generated data\n",
      "foreca does not have generated data\n",
      "theguardian does not have generated data\n",
      "mitocw.mit.edu does not have generated data\n",
      "snagajob does not have generated data\n",
      "yale.edu does not have generated data\n",
      "finance.yahoo does not have generated data\n",
      "khanacademy does not have generated data\n",
      "superpages does not have generated data\n",
      "weather does not have generated data\n",
      "snapchat does not have generated data\n",
      "goal does not have generated data\n",
      "reuters does not have generated data\n",
      "jstor does not have generated data\n",
      "twitter does not have generated data\n",
      "flickr does not have generated data\n",
      "usa does not have generated data\n",
      "nba does not have generated data\n",
      "whitepages does not have generated data\n",
      "yellowbook does not have generated data\n",
      "meteoblue does not have generated data\n",
      "simplyhired does not have generated data\n",
      "trulia does not have generated data\n",
      "pinterest does not have generated data\n",
      "slickdeals does not have generated data\n",
      "tripadvisor does not have generated data\n",
      "reuters does not have generated data\n",
      "zillow does not have generated data\n",
      "forbes does not have generated data\n",
      "skysports does not have generated data\n",
      "stackexchange does not have generated data\n",
      "walmart does not have generated data\n",
      "nature does not have generated data\n",
      "usajobs does not have generated data\n",
      "hotfrog does not have generated data\n",
      "realtor does not have generated data\n",
      "youtube does not have generated data\n",
      "zacks does not have generated data\n",
      "investing does not have generated data\n",
      "mlb does not have generated data\n",
      "metoffice does not have generated data\n",
      "yelp does not have generated data\n",
      "fool does not have generated data\n",
      "newegg does not have generated data\n",
      "sec does not have generated data\n",
      "homes does not have generated data\n",
      "quora does not have generated data\n",
      "weather does not have generated data\n",
      "weatherbug does not have generated data\n",
      "stanfordonline.stanford does not have generated data\n",
      "huffpost does not have generated data\n",
      "monster does not have generated data\n",
      "spoke does not have generated data\n",
      "marketwatch does not have generated data\n",
      "yell does not have generated data\n",
      "glassdoor does not have generated data\n",
      "remax does not have generated data\n",
      "loopnet does not have generated data\n",
      "instagram does not have generated data\n",
      "nhl does not have generated data\n",
      "nasa does not have generated data\n",
      "linkedin does not have generated data\n",
      "Dataset length: 384\n"
     ]
    }
   ],
   "source": [
    "# Combine all scraping tasks and solutions into one file\n",
    "\n",
    "# Load prompts\n",
    "system_prompt = open('prompts/generate_solution_code_system_prompt.txt', 'r').read()\n",
    "user_prompt = open('prompts/generate_solution_code_user_prompt.txt', 'r').read()\n",
    "\n",
    "# Load extracted info\n",
    "extracted_info = {}\n",
    "for filename in os.listdir('extracted_info'):\n",
    "    with open(f'extracted_info/{filename}', 'r') as f:\n",
    "        extracted_info[filename.split('.')[0]] = f.read()\n",
    "    \n",
    "# Load scraping tasks\n",
    "scraping_tasks = {}\n",
    "for filename in os.listdir('scraping_tasks'):\n",
    "    with open(f'scraping_tasks/{filename}', 'r') as f:\n",
    "        scraping_tasks[filename.split('.')[0]] = f.readlines()\n",
    "\n",
    "# Load solutions\n",
    "solutions = {}\n",
    "for folder in os.listdir('solution_code'):\n",
    "    solutions[folder] = {}\n",
    "    for filename in os.listdir(f'solution_code/{folder}'):\n",
    "        with open(f'solution_code/{folder}/{filename}', 'r') as f:\n",
    "            solutions[folder][filename.split('.')[0]] = f.read()\n",
    "\n",
    "# Load website infos\n",
    "websites = pd.read_csv('websites.csv')\n",
    "\n",
    "# Generate prompts\n",
    "prompts = {} # Dictionary of dictionaries\n",
    "categories = {}\n",
    "links = {}\n",
    "for i, row in websites.iterrows():\n",
    "    category, website, link = row\n",
    "    categories[website] = category\n",
    "    links[website] = link\n",
    "    if website not in scraping_tasks:\n",
    "        print(f'{website} does not have generated data')\n",
    "        continue\n",
    "    scraping_tasks_website = scraping_tasks[website]\n",
    "    HTML_file = f'downloaded_pages/{website}.html'\n",
    "    prompts[website] = {}\n",
    "    for j, scraping_task in enumerate(scraping_tasks_website):\n",
    "        scraping_task = scraping_task.strip()\n",
    "        solution = solutions[website][f'{website}_{j}']\n",
    "        HTML_elements = extracted_info[website]\n",
    "        prompts[website][f'{website}_{j}'] = user_prompt.format(website=website, HTML_file=HTML_file, category=category, HTML_string=HTML_elements, task=scraping_task)\n",
    "\n",
    "# Generate training samples\n",
    "training_samples = {}\n",
    "sample_template = \"\"\"### System:\n",
    "{system_prompt}\n",
    "\n",
    "### User:\n",
    "{user_prompt}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "{response}\n",
    "```\n",
    "\"\"\"\n",
    "system_prompt = open('prompts/generate_solution_code_system_prompt.txt', 'r').read()\n",
    "for website in prompts:\n",
    "    training_samples[website] = {}\n",
    "    for task_name in prompts[website]:\n",
    "        sample = sample_template.format(system_prompt=system_prompt, user_prompt=prompts[website][task_name], response=solutions[website][task_name])\n",
    "        training_samples[website][task_name] = sample\n",
    "\n",
    "# Convert prompts, solutions and training samples to lists\n",
    "websites_list = []\n",
    "task_names_list = []\n",
    "categories_list = []\n",
    "links_list = []\n",
    "prompts_list = []\n",
    "solutions_list = []\n",
    "training_samples_list = []\n",
    "for website in prompts:\n",
    "    for task_name in prompts[website]:\n",
    "        websites_list.append(website)\n",
    "        task_names_list.append(task_name)\n",
    "        categories_list.append(categories[website])\n",
    "        links_list.append(links[website])\n",
    "        prompts_list.append(prompts[website][task_name])\n",
    "        solutions_list.append(solutions[website][task_name])\n",
    "        training_samples_list.append(training_samples[website][task_name])\n",
    "\n",
    "# Combine to a list of dictionaries\n",
    "data = []\n",
    "for i in range(len(prompts_list)):\n",
    "    data.append({\n",
    "        'website': websites_list[i],\n",
    "        'task': task_names_list[i],\n",
    "        'category': categories_list[i],\n",
    "        'link': links_list[i],\n",
    "        'prompt': prompts_list[i],\n",
    "        'solution': solutions_list[i],\n",
    "        'training_sample': training_samples_list[i]\n",
    "    })\n",
    "print(\"Dataset length:\", len(data))\n",
    "\n",
    "# Save prompts, solutions and training samples as a json file\n",
    "# where each 'row' is a prompt, solution and training sample\n",
    "# for a particular website and task\n",
    "with open('dataset.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample: ### System:\n",
      "When asked to write a script, then write just the code, and nothing else. Don't write any explanation, comments, or disclaimers.\n",
      "\n",
      "### User:\n",
      "You are given a web page, the category of the page, randomly selected html elements on that page, the local path to the HTML file that should be scraped and a web-scraping task that you should solve.\n",
      "\n",
      "Here are some randomly selected HTML elements (containing text), and their corresponding XPaths from the target page:\n",
      "<title>(83) Update on $50k NVDA Puts : wallstreetbets</title>\n",
      "/html/head/title\n",
      "----------------\n",
      "<span>Flipping at the Grand Exchange</span>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[2]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\n",
      "----------------\n",
      "<span class=\"_1RIl585IYPW6cmNXwgRz0J\">User account menu</span>\n",
      "/html/body/div[1]/div/div[2]/div[1]/header/div/div[2]/div[2]/div/div[2]/button/span[2]\n",
      "----------------\n",
      "<a class=\"_3t5uN8xUmg0TOwRCOGQEcU\">r/wallstreetbets</a> sir, not \n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[47]/div/div/div/div[2]/div[2]/div[2]/div/p[1]/a[1]\n",
      "----------------\n",
      "<a class=\"zrXDKcys3Vl7vt1f6ef4V\">Privacy Policy</a> .\n",
      "/html/body/div[1]/div/div[2]/div[3]/div/section/div/section[1]/div/span[3]/a[2]\n",
      "----------------\n",
      "<div class=\"tbIApBd2DM_drfZQJjIum\">Screenshots of Social Media Posts are Prohibited</div>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[2]/div/div[2]/div\n",
      "----------------\n",
      "<div class=\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\">1</div>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[213]/div/div/div/div[2]/div[2]/div[3]/div[1]/div\n",
      "----------------\n",
      "<h1 class=\"_eYtD2XCVieq6emjKBH3m _2SdHzo12ISmrC8H86TgSCp _29WrubtjAcKqzJSPdQqQ4h\">Update on $50k NVDA Puts</h1>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[1]/div/div/div/div[3]/div[1]/div/h1\n",
      "----------------\n",
      "<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Lol that chip ban was a goddamn lifeboat from the </p>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[167]/div/div/div/div[2]/div[2]/div[2]/div/p\n",
      "----------------\n",
      "<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Cramer says Calls</p>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[173]/div/div/div/div[2]/div[2]/div[2]/div/p[1]\n",
      "----------------\n",
      "<td>MSFT</td>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[7]/td[8]\n",
      "----------------\n",
      "<h2 class=\"_eYtD2XCVieq6emjKBH3m\">About Community</h2>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[1]/div[1]/div[1]/h2\n",
      "----------------\n",
      "<th>-</th>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/thead/tr/th[5]\n",
      "----------------\n",
      "<span class=\"_1RIl585IYPW6cmNXwgRz0J\">Search within r/wallstreetbets</span>\n",
      "/html/body/div[1]/div/div[2]/div[1]/header/div/div[1]/div[3]/div/form/label/span\n",
      "----------------\n",
      "<span class=\"reddit-actionButton\">Tip</span>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[205]/div/div/div/div[2]/div[2]/div[3]/div[2]/div[2]/template/button/span[2]/span\n",
      "----------------\n",
      "<a class=\"_1WUTKdOO96akYfbq4CK6z6\">View discussions in 1 other community</a>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[4]/a\n",
      "----------------\n",
      "<a class=\"wM6scouPXXsFDSZmZPHRo DjcdNGtVXPcxG0yiFXIoZ _23wugcdiaj44hdfugIAlnX\">Unknownirish</a>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[3]/div/div/div/div[2]/div[2]/div[1]/span/div/div/div/a\n",
      "----------------\n",
      "<div class=\"tbIApBd2DM_drfZQJjIum\">Only Crypto Allowed is BTC and ETH</div>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[8]/div/div[2]/div\n",
      "----------------\n",
      "<div class=\"ULWj94BYSOqoJDetxgcnU\">u/ShopBitter</div>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[2]/div[2]/a/div\n",
      "----------------\n",
      "<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">How do you even give gold now? I don’t see it anyw</p>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[15]/div/div/div/div[2]/div[2]/div[2]/div/p\n",
      "----------------\n",
      "<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Nice</p>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[209]/div/div/div/div[2]/div[2]/div[2]/div/p\n",
      "----------------\n",
      "<td>453</td>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[2]/td[2]\n",
      "----------------\n",
      "<h2 class=\"_eYtD2XCVieq6emjKBH3m\">Moderators</h2>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[1]/div/h2\n",
      "----------------\n",
      "<span>Ask me to rap (WSB's Discount Tupac)</span>\n",
      "/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[8]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\n",
      "----------------\n",
      "<span class=\"reddit-actionButton\">Tip</span>\n",
      "Do NOT include these directly in your code!\n",
      "\n",
      "The page is from this website: reddit\n",
      "\n",
      "The local path to the HTML file is downloaded_pages/reddit.html\n",
      "\n",
      "The category is: Social Media\n",
      "\n",
      "The task is: Extract all the titles from the discussion posts and save them in a CSV file.\n",
      "\n",
      "Now generate a python script that solves this task.\n",
      "Make sure the script works when I run it, without any modifications.\n",
      "The script you generate should always save the scraped data as a CSV file with the name 'scraped_data.csv'.\n",
      "You can use the given HTML elements above to gain information about the page.\n",
      "\n",
      "### Response:\n",
      "```\n",
      "import csv\n",
      "from lxml import etree\n",
      "\n",
      "def extract_titles(html_file):\n",
      "    tree = etree.parse(html_file)\n",
      "    titles = tree.xpath(\"//h3[@class='title']/a/text()\")\n",
      "    return titles\n",
      "\n",
      "def save_to_csv(data, csv_file):\n",
      "    with open(csv_file, 'w', newline='') as file:\n",
      "        writer = csv.writer(file)\n",
      "        writer.writerow(['Title'])\n",
      "        writer.writerows(data)\n",
      "\n",
      "html_file = 'downloaded_pages/reddit.html'\n",
      "csv_file = 'scraped_data.csv'\n",
      "\n",
      "titles = extract_titles(html_file)\n",
      "save_to_csv(titles, csv_file)\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_data = json.load(open('dataset.json', 'r'))\n",
    "# print(\"Loaded data length:\", len(loaded_data))\n",
    "# print(\"Website:\", loaded_data[0]['website'])\n",
    "# print(\"Task:\", loaded_data[0]['task'])\n",
    "# print(\"Category:\", loaded_data[0]['category'])\n",
    "# print(\"Link:\", loaded_data[0]['link'])\n",
    "# print(\"Prompt:\\n\" + loaded_data[0]['prompt'])\n",
    "# print(\"Solution:\", loaded_data[0]['solution'])\n",
    "print(\"Training sample:\", loaded_data[0]['training_sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'You are given a web page, the category of the page, randomly selected html elements on that page, the local path to the HTML file that should be scraped and a web-scraping task that you should solve.\\n\\nHere are some randomly selected HTML elements (containing text), and their corresponding XPaths from the target page:\\n<title>(83) Update on $50k NVDA Puts : wallstreetbets</title>\\n/html/head/title\\n----------------\\n<span>Flipping at the Grand Exchange</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[2]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\\n----------------\\n<span class=\"_1RIl585IYPW6cmNXwgRz0J\">User account menu</span>\\n/html/body/div[1]/div/div[2]/div[1]/header/div/div[2]/div[2]/div/div[2]/button/span[2]\\n----------------\\n<a class=\"_3t5uN8xUmg0TOwRCOGQEcU\">r/wallstreetbets</a> sir, not \\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[47]/div/div/div/div[2]/div[2]/div[2]/div/p[1]/a[1]\\n----------------\\n<a class=\"zrXDKcys3Vl7vt1f6ef4V\">Privacy Policy</a> .\\n/html/body/div[1]/div/div[2]/div[3]/div/section/div/section[1]/div/span[3]/a[2]\\n----------------\\n<div class=\"tbIApBd2DM_drfZQJjIum\">Screenshots of Social Media Posts are Prohibited</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[2]/div/div[2]/div\\n----------------\\n<div class=\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\">1</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[213]/div/div/div/div[2]/div[2]/div[3]/div[1]/div\\n----------------\\n<h1 class=\"_eYtD2XCVieq6emjKBH3m _2SdHzo12ISmrC8H86TgSCp _29WrubtjAcKqzJSPdQqQ4h\">Update on $50k NVDA Puts</h1>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[1]/div/div/div/div[3]/div[1]/div/h1\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Lol that chip ban was a goddamn lifeboat from the </p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[167]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Cramer says Calls</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[173]/div/div/div/div[2]/div[2]/div[2]/div/p[1]\\n----------------\\n<td>MSFT</td>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[7]/td[8]\\n----------------\\n<h2 class=\"_eYtD2XCVieq6emjKBH3m\">About Community</h2>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[1]/div[1]/div[1]/h2\\n----------------\\n<th>-</th>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/thead/tr/th[5]\\n----------------\\n<span class=\"_1RIl585IYPW6cmNXwgRz0J\">Search within r/wallstreetbets</span>\\n/html/body/div[1]/div/div[2]/div[1]/header/div/div[1]/div[3]/div/form/label/span\\n----------------\\n<span class=\"reddit-actionButton\">Tip</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[205]/div/div/div/div[2]/div[2]/div[3]/div[2]/div[2]/template/button/span[2]/span\\n----------------\\n<a class=\"_1WUTKdOO96akYfbq4CK6z6\">View discussions in 1 other community</a>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[4]/a\\n----------------\\n<a class=\"wM6scouPXXsFDSZmZPHRo DjcdNGtVXPcxG0yiFXIoZ _23wugcdiaj44hdfugIAlnX\">Unknownirish</a>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[3]/div/div/div/div[2]/div[2]/div[1]/span/div/div/div/a\\n----------------\\n<div class=\"tbIApBd2DM_drfZQJjIum\">Only Crypto Allowed is BTC and ETH</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[8]/div/div[2]/div\\n----------------\\n<div class=\"ULWj94BYSOqoJDetxgcnU\">u/ShopBitter</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[2]/div[2]/a/div\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">How do you even give gold now? I don’t see it anyw</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[15]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Nice</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[209]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<td>453</td>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[2]/td[2]\\n----------------\\n<h2 class=\"_eYtD2XCVieq6emjKBH3m\">Moderators</h2>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[1]/div/h2\\n----------------\\n<span>Ask me to rap (WSB\\'s Discount Tupac)</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[8]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\\n----------------\\n<span class=\"reddit-actionButton\">Tip</span>\\nDo NOT include these directly in your code!\\n\\nThe page is from this website: reddit\\n\\nThe local path to the HTML file is downloaded_pages/reddit.html\\n\\nThe category is: Forums and Review Sites\\n\\nThe task is: Extract all the titles from the discussion posts and save them in a CSV file.\\n\\nNow generate a python script that solves this task.\\nMake sure the script works when I run it, without any modifications.\\nThe script you generate should always save the scraped data as a CSV file with the name \\'scraped_data.csv\\'.\\nYou can use the given HTML elements above to gain information about the page.',\n",
       " 'solution': 'import csv\\nfrom lxml import etree\\n\\ndef extract_titles(html_file):\\n    tree = etree.parse(html_file)\\n    titles = tree.xpath(\"//h3[@class=\\'title\\']/a/text()\")\\n    return titles\\n\\ndef save_to_csv(data, csv_file):\\n    with open(csv_file, \\'w\\', newline=\\'\\') as file:\\n        writer = csv.writer(file)\\n        writer.writerow([\\'Title\\'])\\n        writer.writerows(data)\\n\\nhtml_file = \\'downloaded_pages/reddit.html\\'\\ncsv_file = \\'scraped_data.csv\\'\\n\\ntitles = extract_titles(html_file)\\nsave_to_csv(titles, csv_file)',\n",
       " 'training_sample': '### System:\\nWhen asked to write a script, then write just the code, and nothing else. Don\\'t write any explanation, comments, or disclaimers.\\n\\n### User:\\nYou are given a web page, the category of the page, randomly selected html elements on that page, the local path to the HTML file that should be scraped and a web-scraping task that you should solve.\\n\\nHere are some randomly selected HTML elements (containing text), and their corresponding XPaths from the target page:\\n<title>(83) Update on $50k NVDA Puts : wallstreetbets</title>\\n/html/head/title\\n----------------\\n<span>Flipping at the Grand Exchange</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[2]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\\n----------------\\n<span class=\"_1RIl585IYPW6cmNXwgRz0J\">User account menu</span>\\n/html/body/div[1]/div/div[2]/div[1]/header/div/div[2]/div[2]/div/div[2]/button/span[2]\\n----------------\\n<a class=\"_3t5uN8xUmg0TOwRCOGQEcU\">r/wallstreetbets</a> sir, not \\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[47]/div/div/div/div[2]/div[2]/div[2]/div/p[1]/a[1]\\n----------------\\n<a class=\"zrXDKcys3Vl7vt1f6ef4V\">Privacy Policy</a> .\\n/html/body/div[1]/div/div[2]/div[3]/div/section/div/section[1]/div/span[3]/a[2]\\n----------------\\n<div class=\"tbIApBd2DM_drfZQJjIum\">Screenshots of Social Media Posts are Prohibited</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[2]/div/div[2]/div\\n----------------\\n<div class=\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\">1</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[213]/div/div/div/div[2]/div[2]/div[3]/div[1]/div\\n----------------\\n<h1 class=\"_eYtD2XCVieq6emjKBH3m _2SdHzo12ISmrC8H86TgSCp _29WrubtjAcKqzJSPdQqQ4h\">Update on $50k NVDA Puts</h1>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[1]/div/div/div/div[3]/div[1]/div/h1\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Lol that chip ban was a goddamn lifeboat from the </p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[167]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Cramer says Calls</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[173]/div/div/div/div[2]/div[2]/div[2]/div/p[1]\\n----------------\\n<td>MSFT</td>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[7]/td[8]\\n----------------\\n<h2 class=\"_eYtD2XCVieq6emjKBH3m\">About Community</h2>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[1]/div[1]/div[1]/h2\\n----------------\\n<th>-</th>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/thead/tr/th[5]\\n----------------\\n<span class=\"_1RIl585IYPW6cmNXwgRz0J\">Search within r/wallstreetbets</span>\\n/html/body/div[1]/div/div[2]/div[1]/header/div/div[1]/div[3]/div/form/label/span\\n----------------\\n<span class=\"reddit-actionButton\">Tip</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[205]/div/div/div/div[2]/div[2]/div[3]/div[2]/div[2]/template/button/span[2]/span\\n----------------\\n<a class=\"_1WUTKdOO96akYfbq4CK6z6\">View discussions in 1 other community</a>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[4]/a\\n----------------\\n<a class=\"wM6scouPXXsFDSZmZPHRo DjcdNGtVXPcxG0yiFXIoZ _23wugcdiaj44hdfugIAlnX\">Unknownirish</a>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[3]/div/div/div/div[2]/div[2]/div[1]/span/div/div/div/a\\n----------------\\n<div class=\"tbIApBd2DM_drfZQJjIum\">Only Crypto Allowed is BTC and ETH</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[4]/div/div[2]/div[8]/div/div[2]/div\\n----------------\\n<div class=\"ULWj94BYSOqoJDetxgcnU\">u/ShopBitter</div>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[2]/div[2]/a/div\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">How do you even give gold now? I don’t see it anyw</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[15]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Nice</p>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[209]/div/div/div/div[2]/div[2]/div[2]/div/p\\n----------------\\n<td>453</td>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[3]/div/div[2]/div/div/div/table/tbody/tr[2]/td[2]\\n----------------\\n<h2 class=\"_eYtD2XCVieq6emjKBH3m\">Moderators</h2>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/div[5]/div/div[1]/div/h2\\n----------------\\n<span>Ask me to rap (WSB\\'s Discount Tupac)</span>\\n/html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[3]/div[1]/div[3]/div[5]/div/div/div/div[8]/div/div/div/div[2]/div[2]/div[1]/span[2]/div/span\\n----------------\\n<span class=\"reddit-actionButton\">Tip</span>\\nDo NOT include these directly in your code!\\n\\nThe page is from this website: reddit\\n\\nThe local path to the HTML file is downloaded_pages/reddit.html\\n\\nThe category is: Forums and Review Sites\\n\\nThe task is: Extract all the titles from the discussion posts and save them in a CSV file.\\n\\nNow generate a python script that solves this task.\\nMake sure the script works when I run it, without any modifications.\\nThe script you generate should always save the scraped data as a CSV file with the name \\'scraped_data.csv\\'.\\nYou can use the given HTML elements above to gain information about the page.\\n\\n### Response:\\n```\\nimport csv\\nfrom lxml import etree\\n\\ndef extract_titles(html_file):\\n    tree = etree.parse(html_file)\\n    titles = tree.xpath(\"//h3[@class=\\'title\\']/a/text()\")\\n    return titles\\n\\ndef save_to_csv(data, csv_file):\\n    with open(csv_file, \\'w\\', newline=\\'\\') as file:\\n        writer = csv.writer(file)\\n        writer.writerow([\\'Title\\'])\\n        writer.writerows(data)\\n\\nhtml_file = \\'downloaded_pages/reddit.html\\'\\ncsv_file = \\'scraped_data.csv\\'\\n\\ntitles = extract_titles(html_file)\\nsave_to_csv(titles, csv_file)\\n```\\n'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
