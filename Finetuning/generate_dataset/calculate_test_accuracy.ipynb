{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the code from the responses\n",
    "import regex as re\n",
    "\n",
    "def extract_code(response):\n",
    "    scraper_code = re.search(r\"(?<=```\\n)[\\s\\S]+?(?=```)\", response)\n",
    "    if scraper_code != None:\n",
    "        scraper_code = scraper_code.group(0)\n",
    "    else:\n",
    "        scraper_code = response\n",
    "    return scraper_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating the CSV files for the test set\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def run_scripts(csv_folder, solution_code_folder):\n",
    "    err_dict = {}\n",
    "    for filename in os.listdir(solution_code_folder):\n",
    "        if filename.endswith('.py'):\n",
    "            import_line = f'from {solution_code_folder.replace(\"/\", \".\")} import {filename[:-3]}'\n",
    "            print(import_line)\n",
    "\n",
    "            try:\n",
    "                exec(import_line)\n",
    "            except:\n",
    "                err_dict[filename[:-3]] = 0\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv('scraped_data.csv')\n",
    "            except:\n",
    "                err_dict[filename[:-3]] = 1\n",
    "                continue\n",
    "\n",
    "            err_dict[filename[:-3]] = 2\n",
    "\n",
    "            # Save the CSV file\n",
    "            df.to_csv(f'{csv_folder}/{filename[:-3]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for jaccard, dice and overlap similarity\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "def dice_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    return float(2 * intersection) / (len(list1) + len(list2))\n",
    "\n",
    "def overlap_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    return float(intersection) / min(len(list1), len(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating score of model for each script in the test set\n",
    "def calculate_test_accuracy(generated_csv_folder, correct_csv_folder):\n",
    "    # Calculate the score for each script\n",
    "    score_dict = {}\n",
    "    for filename in os.listdir(correct_csv_folder):\n",
    "        generated_csv = pd.read_csv(f'{generated_csv_folder}/{filename}')\n",
    "        correct_csv = pd.read_csv(f'{correct_csv_folder}/{filename}')\n",
    "\n",
    "        # Load all columns of each CSV file and store each column in a list in a dictionary\n",
    "        correct_dict = {}\n",
    "        generated_dict = {}\n",
    "        for column in correct_csv.columns:\n",
    "            correct_dict[column] = correct_csv[column].tolist()\n",
    "            generated_dict[column] = generated_csv[column].tolist()\n",
    "        \n",
    "        # For each correct column, find the generated column with the highest similarity score\n",
    "        total_score = 0\n",
    "        for column in correct_dict.keys():\n",
    "            max_score = 0\n",
    "            for column2 in generated_dict.keys():\n",
    "                max_score = max(max_score, jaccard_similarity(correct_dict[column], generated_dict[column2]))\n",
    "            total_score += max_score\n",
    "\n",
    "        # Calculate the average score for this csv file\n",
    "        score_dict[filename[:-3]] = total_score / len(correct_dict.keys())\n",
    "\n",
    "    # Save the score dictionary as a JSON file\n",
    "    with open('score_dict.json', 'w') as f:\n",
    "        json.dump(score_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating Mann-Whitney U test for finding significance of difference in mean between two models\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def calculate_mann_whitney_u_test(model_name1, model_name2):\n",
    "    # Load the score dictionary for each model\n",
    "    with open(f'score_dict_{model_name1}.json', 'r') as f:\n",
    "        score_dict_model1 = json.load(f)\n",
    "    with open(f'score_dict_{model_name2}.json', 'r') as f:\n",
    "        score_dict_model2 = json.load(f)\n",
    "    \n",
    "    # Get the two lists of scores\n",
    "    scores_model1 = score_dict_model1.values()\n",
    "    scores_model2 = score_dict_model2.values()\n",
    "\n",
    "    # Calculate the Mann-Whitney U test\n",
    "    stat, p = mannwhitneyu(scores_model1, scores_model2)\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "    # Interpretation of the test\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print(f'Fail to reject H0: {model_name1} and {model_name2} have the same distribution')\n",
    "    else:\n",
    "        print(f'Reject H0: {model_name1} and {model_name2} have different distributions')\n",
    "\n",
    "\n",
    "def mean_score_of_model(model_name):\n",
    "    # Load the score dictionary for each model\n",
    "    with open(f'score_dict_{model_name}.json', 'r') as f:\n",
    "        score_dict_model = json.load(f)\n",
    "    \n",
    "    # Get the list of scores\n",
    "    scores_model = score_dict_model.values()\n",
    "\n",
    "    # Calculate the mean score\n",
    "    mean_score = np.mean(list(scores_model))\n",
    "    print(f'Mean score of {model_name}: {mean_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Should be done for each model folder\n",
    "models_data_folder = \"models_data\"\n",
    "for model_folder in os.listdir(models_data_folder):\n",
    "    model_folder = 'finetuned'\n",
    "    model_folder = f'{models_data_folder}/{model_folder}'\n",
    "    # Extract the code from the responses\n",
    "    generated_responses_folder = f'{model_folder}/generated_responses'\n",
    "    for filename in os.listdir(generated_responses_folder):\n",
    "        with open(f'{generated_responses_folder}/{filename}', 'r') as f:\n",
    "            response = f.read()\n",
    "        code = extract_code(response)\n",
    "        with open(f'{model_folder}/extracted_code/{filename[:-4]}_code.py', 'w') as f:\n",
    "            f.write(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Should be done for each model folder\n",
    "models_data_folder = \"models_data\"\n",
    "for model_folder in os.listdir(models_data_folder):\n",
    "    model_folder = 'finetuned'\n",
    "    # Run the scripts\n",
    "    generated_csv_folder = f'{model_folder}/generated_csv_files'\n",
    "    correct_csv_folder = f'{models_data_folder}/correct_csv_files'\n",
    "    # websites_csv = pd.read_csv('websites_evaluation.csv')\n",
    "    run_scripts(generated_csv_folder, f'{model_folder}/extracted_code')\n",
    "\n",
    "    # Calculate the score for each script\n",
    "    calculate_test_accuracy(generated_csv_folder, correct_csv_folder)\n",
    "\n",
    "# Calculate the Mann-Whitney U test for finding significance of difference in mean between two models\n",
    "model_name1 = 'model1'\n",
    "model_name2 = 'model2'\n",
    "calculate_mann_whitney_u_test(model_name1, model_name2)\n",
    "\n",
    "# Calculate the mean score of each model\n",
    "mean_score_of_model(model_name1)\n",
    "mean_score_of_model(model_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of strings to multiple txt files\n",
    "import json\n",
    "\n",
    "with open('generated_responses.json', 'r') as f:\n",
    "    generated_responses = json.load(f)\n",
    "\n",
    "responses_list = generated_responses['finetuned_seed1']\n",
    "for i in range(len(responses_list)):\n",
    "    with open(f'models_data/finetuned/generated_responses/response_{i}.txt', 'w') as f:\n",
    "        f.write(responses_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from solution_code_human.imdb import imdb_0\n",
      "from solution_code_human.imdb import imdb_1\n",
      "from solution_code_human.imdb import imdb_2\n",
      "from solution_code_human.imdb import imdb_3\n",
      "from solution_code_human.imdb import imdb_4\n",
      "from solution_code_human.imdb import imdb_5\n",
      "from solution_code_human.imdb import imdb_6\n",
      "from solution_code_human.imdb import imdb_7\n",
      "from solution_code_human.imdb import imdb_8\n",
      "from solution_code_human.imdb import imdb_9\n"
     ]
    }
   ],
   "source": [
    "# Generate correct CSV files\n",
    "import os\n",
    "\n",
    "\n",
    "run_scripts('models_data/correct_csv_files', 'solution_code_human/imdb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
