{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the code from the responses\n",
    "import regex as re\n",
    "\n",
    "def extract_code(response):\n",
    "    response = \"```python\\n\" + response\n",
    "    scraper_code = re.search(r\"(?<=```python\\n)[\\s\\S]+?(?=```)\", response)\n",
    "    if scraper_code != None:\n",
    "        scraper_code = scraper_code.group(0)\n",
    "    else:\n",
    "        scraper_code = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating the CSV files for the test set\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def run_scripts(websites_csv, csv_folder):\n",
    "    err_dict = {}\n",
    "    for folder in os.listdir('solution_code'):\n",
    "        # If the folder is not in the csv file, skip it\n",
    "        if folder not in websites_csv['website'].values:\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(f'solution_code/{folder}'):\n",
    "            import_line = f'from solution_code.{folder} import {filename[:-3]}'\n",
    "            print(import_line)\n",
    "\n",
    "            try:\n",
    "                exec(import_line)\n",
    "            except:\n",
    "                err_dict[filename[:-3]] = 0\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv('scraped_data.csv')\n",
    "            except:\n",
    "                err_dict[filename[:-3]] = 1\n",
    "                continue\n",
    "\n",
    "            # Save the CSV file\n",
    "            df.to_csv(f'{csv_folder}/{filename[:-3]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for jaccard, dice and overlap similarity\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "def dice_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    return float(2 * intersection) / (len(list1) + len(list2))\n",
    "\n",
    "def overlap_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    return float(intersection) / min(len(list1), len(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating score of model for each script in the test set\n",
    "def calculate_test_accuracy(generated_csv_folder, correct_csv_folder):\n",
    "    # Calculate the score for each script\n",
    "    score_dict = {}\n",
    "    for filename in os.listdir(correct_csv_folder):\n",
    "        generated_csv = pd.read_csv(f'{generated_csv_folder}/{filename}')\n",
    "        correct_csv = pd.read_csv(f'{correct_csv_folder}/{filename}')\n",
    "\n",
    "        # Load all columns of each CSV file and store each column in a list in a dictionary\n",
    "        correct_dict = {}\n",
    "        generated_dict = {}\n",
    "        for column in correct_csv.columns:\n",
    "            correct_dict[column] = correct_csv[column].tolist()\n",
    "            generated_dict[column] = generated_csv[column].tolist()\n",
    "        \n",
    "        # For each correct column, find the generated column with the highest similarity score\n",
    "        total_score = 0\n",
    "        for column in correct_dict.keys():\n",
    "            max_score = 0\n",
    "            for column2 in generated_dict.keys():\n",
    "                max_score = max(max_score, jaccard_similarity(correct_dict[column], generated_dict[column2]))\n",
    "            total_score += max_score\n",
    "\n",
    "        # Calculate the average score for this csv file\n",
    "        score_dict[filename[:-3]] = total_score / len(correct_dict.keys())\n",
    "\n",
    "    # Save the score dictionary as a JSON file\n",
    "    with open('score_dict.json', 'w') as f:\n",
    "        json.dump(score_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating Mann-Whitney U test for finding significance of difference in mean between two models\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def calculate_mann_whitney_u_test(model_name1, model_name2):\n",
    "    # Load the score dictionary for each model\n",
    "    with open(f'score_dict_{model_name1}.json', 'r') as f:\n",
    "        score_dict_model1 = json.load(f)\n",
    "    with open(f'score_dict_{model_name2}.json', 'r') as f:\n",
    "        score_dict_model2 = json.load(f)\n",
    "    \n",
    "    # Get the two lists of scores\n",
    "    scores_model1 = score_dict_model1.values()\n",
    "    scores_model2 = score_dict_model2.values()\n",
    "\n",
    "    # Calculate the Mann-Whitney U test\n",
    "    stat, p = mannwhitneyu(scores_model1, scores_model2)\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "    # Interpretation of the test\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print(f'Fail to reject H0: {model_name1} and {model_name2} have the same distribution')\n",
    "    else:\n",
    "        print(f'Reject H0: {model_name1} and {model_name2} have different distributions')\n",
    "\n",
    "\n",
    "def mean_score_of_model(model_name):\n",
    "    # Load the score dictionary for each model\n",
    "    with open(f'score_dict_{model_name}.json', 'r') as f:\n",
    "        score_dict_model = json.load(f)\n",
    "    \n",
    "    # Get the list of scores\n",
    "    scores_model = score_dict_model.values()\n",
    "\n",
    "    # Calculate the mean score\n",
    "    mean_score = np.mean(list(scores_model))\n",
    "    print(f'Mean score of {model_name}: {mean_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be done for each model folder\n",
    "models_data_folder = \"models_data\"\n",
    "for model_folder in os.listdir(models_data_folder):\n",
    "    model_folder = f'{models_data_folder}/{model_folder}'\n",
    "    # Extract the code from the responses\n",
    "    generated_responses_folder = f'{model_folder}/generated_responses'\n",
    "    for filename in os.listdir(generated_responses_folder):\n",
    "        with open(f'{generated_responses_folder}/{filename}', 'r') as f:\n",
    "            response = f.read()\n",
    "        code = extract_code(response)\n",
    "        with open(f'{generated_responses_folder}/{filename[:-5]}_code', 'w') as f:\n",
    "            f.write(code)\n",
    "\n",
    "    # Run the scripts\n",
    "    generated_csv_folder = f'{model_folder}/generated_csv_files'\n",
    "    correct_csv_folder = f'{models_data_folder}/correct_csv_files'\n",
    "    websites_csv = pd.read_csv('websites_evaluation.csv')\n",
    "    run_scripts(websites_csv, generated_csv_folder)\n",
    "\n",
    "    # Calculate the score for each script\n",
    "    calculate_test_accuracy(generated_csv_folder, correct_csv_folder)\n",
    "\n",
    "# Calculate the Mann-Whitney U test for finding significance of difference in mean between two models\n",
    "model_name1 = 'model1'\n",
    "model_name2 = 'model2'\n",
    "calculate_mann_whitney_u_test(model_name1, model_name2)\n",
    "\n",
    "# Calculate the mean score of each model\n",
    "mean_score_of_model(model_name1)\n",
    "mean_score_of_model(model_name2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
