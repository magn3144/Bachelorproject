{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the code from the responses\n",
    "import regex as re\n",
    "\n",
    "def extract_code(response):\n",
    "    scraper_code = re.search(r\"(?<=```)[\\s\\S]+?(?=```)\", response)\n",
    "    if scraper_code != None:\n",
    "        scraper_code = scraper_code.group(0)\n",
    "    else:\n",
    "        scraper_code = response\n",
    "    # check if first line is \"python\"\n",
    "    if scraper_code.startswith(\"python\"):\n",
    "        scraper_code = scraper_code[6:]\n",
    "    return scraper_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating the CSV files for the test set\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def run_scripts(csv_folder, solution_code_folder):\n",
    "    err_dict = {}\n",
    "    for filename in os.listdir(solution_code_folder):\n",
    "        if filename.endswith('.py'):\n",
    "            import_line = f'from {solution_code_folder.replace(\"/\", \".\")} import {filename[:-3]}'\n",
    "            print(import_line)\n",
    "\n",
    "            try:\n",
    "                exec(import_line)\n",
    "            except:\n",
    "                err_dict[filename[:-3]] = 0\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv('scraped_data.csv')\n",
    "            except:\n",
    "                err_dict[filename[:-3]] = 1\n",
    "                continue\n",
    "\n",
    "            err_dict[filename[:-3]] = 2\n",
    "\n",
    "            # Save the CSV file\n",
    "            df.to_csv(f'{csv_folder}/{filename[:-3]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for jaccard, dice and overlap similarity\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "def dice_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    return float(2 * intersection) / (len(list1) + len(list2))\n",
    "\n",
    "def overlap_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    return float(intersection) / min(len(list1), len(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating score of model for each script in the test set\n",
    "def calculate_test_accuracy(generated_csv_folder, correct_csv_folder):\n",
    "    # Calculate the score for each script\n",
    "    score_dict = {}\n",
    "    for filename in os.listdir(correct_csv_folder):\n",
    "        generated_csv = pd.read_csv(f'{generated_csv_folder}/{filename}')\n",
    "        correct_csv = pd.read_csv(f'{correct_csv_folder}/{filename}')\n",
    "\n",
    "        # Load all columns of each CSV file and store each column in a list in a dictionary\n",
    "        correct_dict = {}\n",
    "        generated_dict = {}\n",
    "        for column in correct_csv.columns:\n",
    "            correct_dict[column] = correct_csv[column].tolist()\n",
    "            generated_dict[column] = generated_csv[column].tolist()\n",
    "        \n",
    "        # For each correct column, find the generated column with the highest similarity score\n",
    "        total_score = 0\n",
    "        for column in correct_dict.keys():\n",
    "            max_score = 0\n",
    "            for column2 in generated_dict.keys():\n",
    "                max_score = max(max_score, jaccard_similarity(correct_dict[column], generated_dict[column2]))\n",
    "            total_score += max_score\n",
    "\n",
    "        # Calculate the average score for this csv file\n",
    "        score_dict[filename[:-3]] = total_score / len(correct_dict.keys())\n",
    "\n",
    "    # Save the score dictionary as a JSON file\n",
    "    with open('score_dict.json', 'w') as f:\n",
    "        json.dump(score_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating Mann-Whitney U test for finding significance of difference in mean between two models\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def calculate_mann_whitney_u_test(model_name1, model_name2):\n",
    "    # Load the score dictionary for each model\n",
    "    with open(f'score_dict_{model_name1}.json', 'r') as f:\n",
    "        score_dict_model1 = json.load(f)\n",
    "    with open(f'score_dict_{model_name2}.json', 'r') as f:\n",
    "        score_dict_model2 = json.load(f)\n",
    "    \n",
    "    # Get the two lists of scores\n",
    "    scores_model1 = score_dict_model1.values()\n",
    "    scores_model2 = score_dict_model2.values()\n",
    "\n",
    "    # Calculate the Mann-Whitney U test\n",
    "    stat, p = mannwhitneyu(scores_model1, scores_model2)\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "    # Interpretation of the test\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print(f'Fail to reject H0: {model_name1} and {model_name2} have the same distribution')\n",
    "    else:\n",
    "        print(f'Reject H0: {model_name1} and {model_name2} have different distributions')\n",
    "\n",
    "\n",
    "def mean_score_of_model(model_name):\n",
    "    # Load the score dictionary for each model\n",
    "    with open(f'score_dict_{model_name}.json', 'r') as f:\n",
    "        score_dict_model = json.load(f)\n",
    "    \n",
    "    # Get the list of scores\n",
    "    scores_model = score_dict_model.values()\n",
    "\n",
    "    # Calculate the mean score\n",
    "    mean_score = np.mean(list(scores_model))\n",
    "    print(f'Mean score of {model_name}: {mean_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Should be done for each model folder\n",
    "models_data_folder = \"models_data\"\n",
    "for model_folder in os.listdir(models_data_folder):\n",
    "    model_folder = 'finetuned'\n",
    "    model_folder = f'{models_data_folder}/{model_folder}'\n",
    "    # Extract the code from the responses\n",
    "    generated_responses_folder = f'{model_folder}/generated_responses'\n",
    "    for filename in os.listdir(generated_responses_folder):\n",
    "        with open(f'{generated_responses_folder}/{filename}', 'r') as f:\n",
    "            response = f.read()\n",
    "        code = extract_code(response)\n",
    "        with open(f'{model_folder}/extracted_code/{filename[:-4]}_code.py', 'w') as f:\n",
    "            f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'finetuned/extracted_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\magnu\\Documents\\GitHub Projects\\Bachelorproject\\Finetuning\\generate_dataset\\calculate_test_accuracy.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/calculate_test_accuracy.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m correct_csv_folder \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodels_data_folder\u001b[39m}\u001b[39;00m\u001b[39m/correct_csv_files\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/calculate_test_accuracy.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# websites_csv = pd.read_csv('websites_evaluation.csv')\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/calculate_test_accuracy.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m run_scripts(generated_csv_folder, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_folder\u001b[39m}\u001b[39;49;00m\u001b[39m/extracted_code\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/calculate_test_accuracy.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Calculate the score for each script\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/calculate_test_accuracy.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m calculate_test_accuracy(generated_csv_folder, correct_csv_folder)\n",
      "\u001b[1;32mc:\\Users\\magnu\\Documents\\GitHub Projects\\Bachelorproject\\Finetuning\\generate_dataset\\calculate_test_accuracy.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/calculate_test_accuracy.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_scripts\u001b[39m(csv_folder, solution_code_folder):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/calculate_test_accuracy.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     err_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/calculate_test_accuracy.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(solution_code_folder):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/calculate_test_accuracy.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/magnu/Documents/GitHub%20Projects/Bachelorproject/Finetuning/generate_dataset/calculate_test_accuracy.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             import_line \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfrom \u001b[39m\u001b[39m{\u001b[39;00msolution_code_folder\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m import \u001b[39m\u001b[39m{\u001b[39;00mfilename[:\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'finetuned/extracted_code'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Make sure the scripts are saved with the correct names\n",
    "\n",
    "# Should be done for each model folder\n",
    "models_data_folder = \"models_data\"\n",
    "for model_folder in os.listdir(models_data_folder):\n",
    "    model_folder = 'finetuned'\n",
    "    # Run the scripts\n",
    "    generated_csv_folder = f'{model_folder}/generated_csv_files'\n",
    "    correct_csv_folder = f'{models_data_folder}/correct_csv_files'\n",
    "    # websites_csv = pd.read_csv('websites_evaluation.csv')\n",
    "    run_scripts(generated_csv_folder, f'{model_folder}/extracted_code')\n",
    "\n",
    "    # Calculate the score for each script\n",
    "    calculate_test_accuracy(generated_csv_folder, correct_csv_folder)\n",
    "\n",
    "# Calculate the Mann-Whitney U test for finding significance of difference in mean between two models\n",
    "model_name1 = 'model1'\n",
    "model_name2 = 'model2'\n",
    "calculate_mann_whitney_u_test(model_name1, model_name2)\n",
    "\n",
    "# Calculate the mean score of each model\n",
    "mean_score_of_model(model_name1)\n",
    "mean_score_of_model(model_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of strings to multiple txt files\n",
    "import json\n",
    "\n",
    "with open('generated_responses.json', 'r') as f:\n",
    "    generated_responses = json.load(f)\n",
    "\n",
    "responses_list = generated_responses['finetuned_lr0.001_e1_r16_seed42']\n",
    "dtu_count = 0\n",
    "airbnb_count = 0\n",
    "imdb_count = 0\n",
    "for i in range(len(responses_list)):\n",
    "    if \"downloaded_pages/airbnb.html\" in responses_list[i]:\n",
    "        filename = f'airbnb_{airbnb_count}.txt'\n",
    "        airbnb_count += 1\n",
    "    elif \"downloaded_pages/DTU_entrepreneurship.html\" in responses_list[i]:\n",
    "        filename = f'DTU_entrepreneurship_{dtu_count}.txt'\n",
    "        dtu_count += 1\n",
    "    elif \"downloaded_pages/imdb.html\" in responses_list[i]:\n",
    "        filename = f'imdb_{imdb_count}.txt'\n",
    "        imdb_count += 1\n",
    "    else:\n",
    "        filename = f'unknown_{i}.txt'\n",
    "\n",
    "    with open(f'models_data/finetuned/generated_responses/{filename}', 'w') as f:\n",
    "        f.write(responses_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from solution_code_human.imdb import imdb_0\n",
      "from solution_code_human.imdb import imdb_1\n",
      "from solution_code_human.imdb import imdb_2\n",
      "from solution_code_human.imdb import imdb_3\n",
      "from solution_code_human.imdb import imdb_4\n",
      "from solution_code_human.imdb import imdb_5\n",
      "from solution_code_human.imdb import imdb_6\n",
      "from solution_code_human.imdb import imdb_7\n",
      "from solution_code_human.imdb import imdb_8\n",
      "from solution_code_human.imdb import imdb_9\n"
     ]
    }
   ],
   "source": [
    "# Generate correct CSV files\n",
    "import os\n",
    "\n",
    "\n",
    "run_scripts('models_data/correct_csv_files', 'solution_code_human/imdb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
