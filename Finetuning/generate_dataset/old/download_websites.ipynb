{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_websites(CSV_name):\n",
    "    # Ensure to create a directory to save the HTML files\n",
    "    if not os.path.exists('downloaded_pages'):\n",
    "        os.makedirs('downloaded_pages')\n",
    "\n",
    "    # Read websites from the CSV file\n",
    "    with open(CSV_name, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header\n",
    "        \n",
    "        for row in reader:\n",
    "            category, website, page, link = row\n",
    "            \n",
    "            try:\n",
    "                # Make a GET request to fetch the raw HTML content\n",
    "                response = requests.get(link, timeout=10)\n",
    "                \n",
    "                # Check if the request was successful (HTTP Status Code 200)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    # Save the HTML content to a file\n",
    "                    with open(f'downloaded_pages/{website}.html', 'w', encoding='utf-8') as html_file:\n",
    "                        html_file.write(str(soup))\n",
    "                    print(f\"Downloaded {website}\")\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve {website}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving {website}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(HTML_files):\n",
    "    \"\"\"Get body from HTML files, and save to new HTML files\"\"\"\n",
    "    # Ensure to create a directory to save the HTML files\n",
    "    if not os.path.exists('body_pages'):\n",
    "        os.makedirs('body_pages')\n",
    "\n",
    "    for HTML_file in HTML_files:\n",
    "        # Read HTML file\n",
    "        with open(f'downloaded_pages/{HTML_file}', 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "            body = soup.find('body')\n",
    "            # Save the HTML content to a file\n",
    "            with open(f'body_pages/{HTML_file}', 'w', encoding='utf-8') as html_file:\n",
    "                html_file.write(str(body))\n",
    "            print(f\"Saved body of {HTML_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv(csv_name, n):\n",
    "    \"\"\"\n",
    "    Split a CSV file into n equally long files.\n",
    "\n",
    "    Parameters:\n",
    "    csv_name (str): The path to the CSV file to split\n",
    "    n (int): The number of equally long CSV files to create\n",
    "    \"\"\"\n",
    "    with open(csv_name, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header\n",
    "        \n",
    "        # Create a list of rows\n",
    "        rows = []\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "\n",
    "        # Split the rows into n equally long lists\n",
    "        rows_per_csv = len(rows) // n\n",
    "        rows_split = [rows[i:i + rows_per_csv] for i in range(0, len(rows), rows_per_csv)]\n",
    "        \n",
    "        # Write each list of rows to a separate CSV file\n",
    "        for i in range(n):\n",
    "            with open(f'split_csv/{csv_name.split(\".\")[0]}_{i}.csv', mode='w', encoding='utf-8', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(header)\n",
    "                writer.writerows(rows_split[i])\n",
    "\n",
    "# Create a directory to save the CSV files\n",
    "if not os.path.exists('split_csv'):\n",
    "    os.makedirs('split_csv')\n",
    "\n",
    "split_csv('websites.csv', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_n_rows(csv_name, n):\n",
    "    \"\"\"\n",
    "    Get the first n rows from a CSV file,\n",
    "    and save them to a new CSV file.\n",
    "    \"\"\"\n",
    "    with open(csv_name, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header\n",
    "        \n",
    "        # Create a list of rows\n",
    "        rows = []\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "        \n",
    "        # Write the first n rows to a new CSV file\n",
    "        with open(f'first_{n}_rows.csv', mode='w', encoding='utf-8', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(rows[:n])\n",
    "\n",
    "get_first_n_rows('websites_full_link_final.csv', 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
